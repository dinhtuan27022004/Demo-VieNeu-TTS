{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "header",
            "metadata": {},
            "source": [
                "# VieNeu TTS Demo\n",
                "\n",
                "This notebook demonstrates text-to-speech synthesis using VieNeu SDK with:\n",
                "1. Preset voices\n",
                "2. Custom voice cloning\n",
                "3. Batch speech synthesis\n",
                "\n",
                "## New: All-in-One Function\n",
                "All functionality has been combined into a single `run_vieneu_tts_batch()` function below."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "id": "all-in-one-function",
            "metadata": {},
            "outputs": [
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "/home/lamquy/miniconda3/envs/vienu/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
                        "  from .autonotebook import tqdm as notebook_tqdm\n",
                        "Skipping import of cpp extensions due to incompatible torch version 2.7.1+cu118 for torchao version 0.14.1             Please see https://github.com/pytorch/ao/issues/2919 for more info\n"
                    ]
                }
            ],
            "source": [
                "# All-in-One VieNeu TTS Function\n",
                "import sys\n",
                "import datetime\n",
                "import soundfile as sf\n",
                "from pathlib import Path\n",
                "from vieneu import Vieneu\n",
                "\n",
                "\n",
                "def run_vieneu_tts_batch(\n",
                "    text_samples=None,\n",
                "    output_base_dir=\"/home/lamquy/Project/TTS/results/VieNeu-TTS\",\n",
                "    notebook_dir=\"/home/lamquy/Project/TTS/VieNeu-TTS\",\n",
                "    preset_voice=\"Binh\",\n",
                "    use_custom_voice=False,\n",
                "    sample_audio_path=None,\n",
                "    sample_audio_text_path=None,\n",
                "    custom_voice_name=\"MyCustomVoice\",\n",
                "    temperature=1.0,\n",
                "    top_k=50,\n",
                "    sample_rate=24000\n",
                "):\n",
                "    \n",
                "    \n",
                "    # ====== SETUP & CONFIGURATION ======\n",
                "    notebook_dir = Path(notebook_dir)\n",
                "    output_base_dir = Path(output_base_dir)\n",
                "    \n",
                "    # Import text samples if not provided\n",
                "    if text_samples is None:\n",
                "        sys.path.insert(0, '/home/lamquy/Project/TTS')\n",
                "        from text_sample import TEXT_SAMPLES\n",
                "        text_samples = TEXT_SAMPLES\n",
                "    \n",
                "    # Set sample audio path defaults\n",
                "    if sample_audio_path is None:\n",
                "        sample_audio_path = notebook_dir / \"examples\" / \"audio_ref\" / \"example.wav\"\n",
                "    else:\n",
                "        sample_audio_path = Path(sample_audio_path)\n",
                "    with open(sample_audio_text_path, \"r\") as f:\n",
                "        sample_audio_text = f.read()\n",
                "    if sample_audio_text_path is None:\n",
                "        sample_audio_text = \"v√≠ d·ª• 2. t√≠nh trung b√¨nh c·ªßa d√£y s·ªë.\"\n",
                "    \n",
                "    # ====== INITIALIZE VieNeu SDK ======\n",
                "    print(\"üöÄ Initializing VieNeu SDK...\")\n",
                "    tts = Vieneu()\n",
                "    print(\"‚úÖ SDK initialized successfully\")\n",
                "    \n",
                "    try:\n",
                "        # ====== SELECT VOICE ======\n",
                "        # List all available preset voices\n",
                "        available_voices = tts.list_preset_voices()\n",
                "        print(\"üìã Available preset voices:\", available_voices)\n",
                "        \n",
                "        # Select a preset voice\n",
                "        current_voice = tts.get_preset_voice(preset_voice)\n",
                "        print(f\"‚úÖ Selected voice: {preset_voice}\")\n",
                "        \n",
                "        # ====== CLONE CUSTOM VOICE (OPTIONAL) ======\n",
                "        if use_custom_voice:\n",
                "            # Check if sample audio exists\n",
                "            if sample_audio_path.exists():\n",
                "                print(f\"üéôÔ∏è Cloning voice from: {sample_audio_path.name}\")\n",
                "                \n",
                "                # Clone voice and save with custom name\n",
                "                custom_voice = tts.clone_voice(\n",
                "                    audio_path=sample_audio_path,\n",
                "                    text=sample_audio_text,\n",
                "                    name=custom_voice_name\n",
                "                )\n",
                "                \n",
                "                print(f\"‚úÖ Voice cloned and saved as: '{custom_voice_name}'\")\n",
                "                \n",
                "                # Switch to the new custom voice\n",
                "                current_voice = custom_voice\n",
                "                \n",
                "                # Verify it was added to the voice list\n",
                "                print(\"üìã Updated voice list:\", tts.list_preset_voices())\n",
                "            else:\n",
                "                print(f\"‚ö†Ô∏è Sample audio not found at: {sample_audio_path}\")\n",
                "                print(\"   Continuing with preset voice...\")\n",
                "        \n",
                "        # ====== BATCH SPEECH SYNTHESIS ======\n",
                "        # Create output directory with timestamp\n",
                "        timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
                "        output_dir = output_base_dir / timestamp\n",
                "        output_dir.mkdir(parents=True, exist_ok=True)\n",
                "        \n",
                "        print(f\"üìÅ Output directory: {output_dir}\")\n",
                "        print(f\"üìù Total samples to process: {len(text_samples)}\")\n",
                "        output = []\n",
                "        # Process each text sample\n",
                "        for idx, text in enumerate(text_samples, start=1):\n",
                "            # Show progress\n",
                "            print(f\"\\nüéß Sample {idx}/{len(text_samples)}: {text[:50]}...\")\n",
                "            \n",
                "            # Generate audio\n",
                "            audio = tts.infer(\n",
                "                text=text,\n",
                "                voice=current_voice,\n",
                "                temperature=temperature,\n",
                "                top_k=top_k\n",
                "            )\n",
                "            \n",
                "            # Save to file\n",
                "            output_file = output_dir / f\"sample_{idx}.wav\"\n",
                "            output.append(output_file)\n",
                "            sf.write(str(output_file), audio, sample_rate)\n",
                "            print(f\"   üíæ Saved: {output_file}\")\n",
                "        \n",
                "        print(\"\\n‚úÖ All samples processed successfully!\")\n",
                "        \n",
                "        return output\n",
                "        \n",
                "    finally:\n",
                "        # ====== CLEANUP ======\n",
                "        # Close the TTS engine\n",
                "        tts.close()\n",
                "        print(\"‚úÖ TTS engine closed\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 2,
            "id": "77ca71e3",
            "metadata": {},
            "outputs": [],
            "source": [
                "def compute_wer_and_plot(\n",
                "    ref_text,\n",
                "    audio_path,\n",
                "    asr_model_name=\"large\",\n",
                "    language=\"vi\"\n",
                "):\n",
                "    \"\"\"\n",
                "    ref_text: str - text chu·∫©n\n",
                "    audio_path: str - path t·ªõi file audio\n",
                "    return: dict (WER + hypothesis + figure)\n",
                "    \"\"\"\n",
                "\n",
                "    # -------- 1. Load ASR --------\n",
                "    model = whisper.load_model(asr_model_name)\n",
                "\n",
                "    # -------- 2. ASR: audio -> hypothesis --------\n",
                "    result = model.transcribe(str(audio_path), language=language)  # ‚úÖ Convert to string\n",
                "    hyp_text = result[\"text\"].strip()\n",
                "\n",
                "    # -------- 3. Normalize --------\n",
                "    ref_text = ref_text.lower().strip()\n",
                "    hyp_text = hyp_text.lower().strip()\n",
                "\n",
                "    # -------- 4. Compute WER --------\n",
                "    wer_score = wer(ref_text, hyp_text)\n",
                "\n",
                "    # -------- 5. Error breakdown --------\n",
                "    details = process_words(ref_text, hyp_text)\n",
                "\n",
                "    error_counts = {\n",
                "        \"Correct\": details.hits,\n",
                "        \"Substitution\": details.substitutions,\n",
                "        \"Deletion\": details.deletions,\n",
                "        \"Insertion\": details.insertions,\n",
                "    }\n",
                "\n",
                "    # -------- 6. Plot --------\n",
                "    fig, ax = plt.subplots()\n",
                "    ax.bar(error_counts.keys(), error_counts.values())\n",
                "    ax.set_title(\"Word Error Rate Breakdown\")\n",
                "    ax.set_ylabel(\"Count\")\n",
                "    ax.set_xlabel(\"Type\")\n",
                "\n",
                "    plt.tight_layout()\n",
                "\n",
                "    return {\n",
                "        \"reference\": ref_text,\n",
                "        \"hypothesis\": hyp_text,\n",
                "        \"WER\": wer_score,\n",
                "        \"error_breakdown\": error_counts,\n",
                "        \"figure\": fig,\n",
                "    }"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "usage-header",
            "metadata": {},
            "source": [
                "## Usage Examples\n",
                "\n",
                "Run the cell below to execute batch TTS with default settings (preset voice)."
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 3,
            "id": "757545da",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Requirement already satisfied: openai-whisper in /home/lamquy/miniconda3/envs/vienu/lib/python3.11/site-packages (20250625)\n",
                        "Requirement already satisfied: matplotlib in /home/lamquy/miniconda3/envs/vienu/lib/python3.11/site-packages (3.10.8)\n",
                        "Requirement already satisfied: jiwer in /home/lamquy/miniconda3/envs/vienu/lib/python3.11/site-packages (4.0.0)\n",
                        "Requirement already satisfied: more-itertools in /home/lamquy/miniconda3/envs/vienu/lib/python3.11/site-packages (from openai-whisper) (10.8.0)\n",
                        "Requirement already satisfied: numba in /home/lamquy/miniconda3/envs/vienu/lib/python3.11/site-packages (from openai-whisper) (0.62.1)\n",
                        "Requirement already satisfied: numpy in /home/lamquy/miniconda3/envs/vienu/lib/python3.11/site-packages (from openai-whisper) (2.3.4)\n",
                        "Requirement already satisfied: tiktoken in /home/lamquy/miniconda3/envs/vienu/lib/python3.11/site-packages (from openai-whisper) (0.12.0)\n",
                        "Requirement already satisfied: torch in /home/lamquy/miniconda3/envs/vienu/lib/python3.11/site-packages (from openai-whisper) (2.7.1+cu118)\n",
                        "Requirement already satisfied: tqdm in /home/lamquy/miniconda3/envs/vienu/lib/python3.11/site-packages (from openai-whisper) (4.67.1)\n",
                        "Requirement already satisfied: triton>=2 in /home/lamquy/miniconda3/envs/vienu/lib/python3.11/site-packages (from openai-whisper) (3.3.1)\n",
                        "Requirement already satisfied: contourpy>=1.0.1 in /home/lamquy/miniconda3/envs/vienu/lib/python3.11/site-packages (from matplotlib) (1.3.3)\n",
                        "Requirement already satisfied: cycler>=0.10 in /home/lamquy/miniconda3/envs/vienu/lib/python3.11/site-packages (from matplotlib) (0.12.1)\n",
                        "Requirement already satisfied: fonttools>=4.22.0 in /home/lamquy/miniconda3/envs/vienu/lib/python3.11/site-packages (from matplotlib) (4.61.1)\n",
                        "Requirement already satisfied: kiwisolver>=1.3.1 in /home/lamquy/miniconda3/envs/vienu/lib/python3.11/site-packages (from matplotlib) (1.4.9)\n",
                        "Requirement already satisfied: packaging>=20.0 in /home/lamquy/miniconda3/envs/vienu/lib/python3.11/site-packages (from matplotlib) (25.0)\n",
                        "Requirement already satisfied: pillow>=8 in /home/lamquy/miniconda3/envs/vienu/lib/python3.11/site-packages (from matplotlib) (11.3.0)\n",
                        "Requirement already satisfied: pyparsing>=3 in /home/lamquy/miniconda3/envs/vienu/lib/python3.11/site-packages (from matplotlib) (3.2.5)\n",
                        "Requirement already satisfied: python-dateutil>=2.7 in /home/lamquy/miniconda3/envs/vienu/lib/python3.11/site-packages (from matplotlib) (2.9.0.post0)\n",
                        "Requirement already satisfied: click>=8.1.8 in /home/lamquy/miniconda3/envs/vienu/lib/python3.11/site-packages (from jiwer) (8.2.1)\n",
                        "Requirement already satisfied: rapidfuzz>=3.9.7 in /home/lamquy/miniconda3/envs/vienu/lib/python3.11/site-packages (from jiwer) (3.14.3)\n",
                        "Requirement already satisfied: six>=1.5 in /home/lamquy/miniconda3/envs/vienu/lib/python3.11/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
                        "Requirement already satisfied: setuptools>=40.8.0 in /home/lamquy/miniconda3/envs/vienu/lib/python3.11/site-packages (from triton>=2->openai-whisper) (80.9.0)\n",
                        "Requirement already satisfied: llvmlite<0.46,>=0.45.0dev0 in /home/lamquy/miniconda3/envs/vienu/lib/python3.11/site-packages (from numba->openai-whisper) (0.45.1)\n",
                        "Requirement already satisfied: regex>=2022.1.18 in /home/lamquy/miniconda3/envs/vienu/lib/python3.11/site-packages (from tiktoken->openai-whisper) (2025.10.23)\n",
                        "Requirement already satisfied: requests>=2.26.0 in /home/lamquy/miniconda3/envs/vienu/lib/python3.11/site-packages (from tiktoken->openai-whisper) (2.32.5)\n",
                        "Requirement already satisfied: charset_normalizer<4,>=2 in /home/lamquy/miniconda3/envs/vienu/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.4.4)\n",
                        "Requirement already satisfied: idna<4,>=2.5 in /home/lamquy/miniconda3/envs/vienu/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken->openai-whisper) (3.11)\n",
                        "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/lamquy/miniconda3/envs/vienu/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2.5.0)\n",
                        "Requirement already satisfied: certifi>=2017.4.17 in /home/lamquy/miniconda3/envs/vienu/lib/python3.11/site-packages (from requests>=2.26.0->tiktoken->openai-whisper) (2025.10.5)\n",
                        "Requirement already satisfied: filelock in /home/lamquy/miniconda3/envs/vienu/lib/python3.11/site-packages (from torch->openai-whisper) (3.20.0)\n",
                        "Requirement already satisfied: typing-extensions>=4.10.0 in /home/lamquy/miniconda3/envs/vienu/lib/python3.11/site-packages (from torch->openai-whisper) (4.15.0)\n",
                        "Requirement already satisfied: sympy>=1.13.3 in /home/lamquy/miniconda3/envs/vienu/lib/python3.11/site-packages (from torch->openai-whisper) (1.14.0)\n",
                        "Requirement already satisfied: networkx in /home/lamquy/miniconda3/envs/vienu/lib/python3.11/site-packages (from torch->openai-whisper) (3.5)\n",
                        "Requirement already satisfied: jinja2 in /home/lamquy/miniconda3/envs/vienu/lib/python3.11/site-packages (from torch->openai-whisper) (3.1.6)\n",
                        "Requirement already satisfied: fsspec in /home/lamquy/miniconda3/envs/vienu/lib/python3.11/site-packages (from torch->openai-whisper) (2025.9.0)\n",
                        "Requirement already satisfied: nvidia-cuda-nvrtc-cu11==11.8.89 in /home/lamquy/miniconda3/envs/vienu/lib/python3.11/site-packages (from torch->openai-whisper) (11.8.89)\n",
                        "Requirement already satisfied: nvidia-cuda-runtime-cu11==11.8.89 in /home/lamquy/miniconda3/envs/vienu/lib/python3.11/site-packages (from torch->openai-whisper) (11.8.89)\n",
                        "Requirement already satisfied: nvidia-cuda-cupti-cu11==11.8.87 in /home/lamquy/miniconda3/envs/vienu/lib/python3.11/site-packages (from torch->openai-whisper) (11.8.87)\n",
                        "Requirement already satisfied: nvidia-cudnn-cu11==9.1.0.70 in /home/lamquy/miniconda3/envs/vienu/lib/python3.11/site-packages (from torch->openai-whisper) (9.1.0.70)\n",
                        "Requirement already satisfied: nvidia-cublas-cu11==11.11.3.6 in /home/lamquy/miniconda3/envs/vienu/lib/python3.11/site-packages (from torch->openai-whisper) (11.11.3.6)\n",
                        "Requirement already satisfied: nvidia-cufft-cu11==10.9.0.58 in /home/lamquy/miniconda3/envs/vienu/lib/python3.11/site-packages (from torch->openai-whisper) (10.9.0.58)\n",
                        "Requirement already satisfied: nvidia-curand-cu11==10.3.0.86 in /home/lamquy/miniconda3/envs/vienu/lib/python3.11/site-packages (from torch->openai-whisper) (10.3.0.86)\n",
                        "Requirement already satisfied: nvidia-cusolver-cu11==11.4.1.48 in /home/lamquy/miniconda3/envs/vienu/lib/python3.11/site-packages (from torch->openai-whisper) (11.4.1.48)\n",
                        "Requirement already satisfied: nvidia-cusparse-cu11==11.7.5.86 in /home/lamquy/miniconda3/envs/vienu/lib/python3.11/site-packages (from torch->openai-whisper) (11.7.5.86)\n",
                        "Requirement already satisfied: nvidia-nccl-cu11==2.21.5 in /home/lamquy/miniconda3/envs/vienu/lib/python3.11/site-packages (from torch->openai-whisper) (2.21.5)\n",
                        "Requirement already satisfied: nvidia-nvtx-cu11==11.8.86 in /home/lamquy/miniconda3/envs/vienu/lib/python3.11/site-packages (from torch->openai-whisper) (11.8.86)\n",
                        "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /home/lamquy/miniconda3/envs/vienu/lib/python3.11/site-packages (from sympy>=1.13.3->torch->openai-whisper) (1.3.0)\n",
                        "Requirement already satisfied: MarkupSafe>=2.0 in /home/lamquy/miniconda3/envs/vienu/lib/python3.11/site-packages (from jinja2->torch->openai-whisper) (3.0.3)\n"
                    ]
                }
            ],
            "source": [
                "!pip install openai-whisper matplotlib jiwer\n"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 4,
            "id": "5a8595dd",
            "metadata": {},
            "outputs": [],
            "source": [
                "import sys\n",
                "import datetime\n",
                "import soundfile as sf\n",
                "from pathlib import Path\n",
                "from vieneu import Vieneu\n",
                "import whisper\n",
                "import matplotlib.pyplot as plt\n",
                "from jiwer import wer, process_words"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 5,
            "id": "ad2631ab",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Whisper version: 20250625\n",
                        "Has load_model: True\n"
                    ]
                }
            ],
            "source": [
                "# Force reload whisper module\n",
                "import importlib\n",
                "import sys\n",
                "if 'whisper' in sys.modules:\n",
                "    del sys.modules['whisper']\n",
                "import whisper\n",
                "# Verify it's the correct whisper\n",
                "print(f\"Whisper version: {whisper.__version__}\")\n",
                "print(f\"Has load_model: {hasattr(whisper, 'load_model')}\")"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 1,
            "id": "c4319f60",
            "metadata": {},
            "outputs": [
                {
                    "ename": "ModuleNotFoundError",
                    "evalue": "No module named 'speechbrain'",
                    "output_type": "error",
                    "traceback": [
                        "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
                        "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
                        "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m      9\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mnn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mfunctional\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mF\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mjiwer\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m wer, cer, process_words, process_characters\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mspeechbrain\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01minference\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mspeaker\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SpeakerRecognition\n\u001b[32m     14\u001b[39m TARGET_SR = \u001b[32m16000\u001b[39m\n\u001b[32m     16\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_load_wav_mono_resample\u001b[39m(path: \u001b[38;5;28mstr\u001b[39m, target_sr: \u001b[38;5;28mint\u001b[39m = TARGET_SR) -> torch.Tensor:\n",
                        "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'speechbrain'"
                    ]
                }
            ],
            "source": [
                "import json\n",
                "import datetime\n",
                "from pathlib import Path\n",
                "\n",
                "import whisper\n",
                "import matplotlib.pyplot as plt\n",
                "import torch\n",
                "import torchaudio\n",
                "import torch.nn.functional as F\n",
                "\n",
                "from jiwer import wer, cer, process_words, process_characters\n",
                "from speechbrain.inference.speaker import SpeakerRecognition\n",
                "\n",
                "TARGET_SR = 16000\n",
                "\n",
                "def _load_wav_mono_resample(path: str, target_sr: int = TARGET_SR) -> torch.Tensor:\n",
                "    wav, sr = torchaudio.load(path)  # [C, T]\n",
                "    if wav.shape[0] > 1:\n",
                "        wav = wav.mean(dim=0, keepdim=True)   # mono\n",
                "    if sr != target_sr:\n",
                "        wav = torchaudio.functional.resample(wav, sr, target_sr)\n",
                "    return wav.float()  # [1, T]\n",
                "\n",
                "class SpeakerSimECAPA:\n",
                "    \"\"\"\n",
                "    Lazy-load ECAPA model 1 l·∫ßn, d√πng l·∫°i nhi·ªÅu l·∫ßn (ƒë·ª° t·∫£i l·∫°i m·ªói file).\n",
                "    \"\"\"\n",
                "    def __init__(self, device: str | None = None):\n",
                "        self.device = device or (\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
                "        self.model = SpeakerRecognition.from_hparams(\n",
                "            source=\"speechbrain/spkrec-ecapa-voxceleb\",\n",
                "            savedir=\"pretrained_models/spkrec-ecapa-voxceleb\",\n",
                "            run_opts={\"device\": self.device},\n",
                "        )\n",
                "\n",
                "    @torch.inference_mode()\n",
                "    def cosine_similarity(self, ref_audio_path: str, gen_audio_path: str) -> float:\n",
                "        ref = _load_wav_mono_resample(ref_audio_path).squeeze(0).unsqueeze(0).to(self.device)  # [1,T]\n",
                "        gen = _load_wav_mono_resample(gen_audio_path).squeeze(0).unsqueeze(0).to(self.device)\n",
                "\n",
                "        emb_ref = self.model.encode_batch(ref).squeeze()\n",
                "        emb_gen = self.model.encode_batch(gen).squeeze()\n",
                "\n",
                "        sim = F.cosine_similarity(emb_ref, emb_gen, dim=0).item()\n",
                "        return float(sim)\n",
                "\n",
                "# Kh·ªüi t·∫°o global instance sau khi class ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a\n",
                "spk_sim = None  # Lazy init ƒë·ªÉ tr√°nh load model khi import\n",
                "\n",
                "def get_speaker_sim_model():\n",
                "    global spk_sim\n",
                "    if spk_sim is None:\n",
                "        spk_sim = SpeakerSimECAPA()\n",
                "    return spk_sim\n",
                "\n",
                "def metric_calculate(\n",
                "    audio_path,\n",
                "    ref_text,\n",
                "    model_name,\n",
                "    asr_model_name=\"large\",\n",
                "    language=\"vi\",\n",
                "    result_dir=\"../results\",\n",
                "    ref_audio_path=None,         # ‚úÖ NEW: audio gi·ªçng g·ªëc ƒë·ªÉ so speaker similarity\n",
                "    speaker_sim_model=None,      # ‚úÖ NEW: truy·ªÅn instance SpeakerSimECAPA ƒë·ªÉ reuse\n",
                "):\n",
                "    \"\"\"\n",
                "    T√≠nh to√°n WER + CER (+ Speaker Similarity n·∫øu c√≥ ref_audio_path) v√† l∆∞u JSON\n",
                "    \"\"\"\n",
                "    # Load ASR model\n",
                "    model = whisper.load_model(asr_model_name)\n",
                "\n",
                "    # Transcribe audio\n",
                "    result = model.transcribe(str(audio_path), language=language)\n",
                "    hyp_text = result[\"text\"].strip()\n",
                "\n",
                "    # Normalize texts\n",
                "    ref_text_normalized = ref_text.lower().strip()\n",
                "    hyp_text_normalized = hyp_text.lower().strip()\n",
                "\n",
                "    # Compute WER + CER\n",
                "    wer_score = wer(ref_text_normalized, hyp_text_normalized)\n",
                "    cer_score = cer(ref_text_normalized, hyp_text_normalized)\n",
                "\n",
                "    # Word-level error breakdown\n",
                "    word_details = process_words(ref_text_normalized, hyp_text_normalized)\n",
                "    word_error_counts = {\n",
                "        \"Correct\": word_details.hits,\n",
                "        \"Substitution\": word_details.substitutions,\n",
                "        \"Deletion\": word_details.deletions,\n",
                "        \"Insertion\": word_details.insertions,\n",
                "    }\n",
                "\n",
                "    # Character-level error breakdown\n",
                "    char_details = process_characters(ref_text_normalized, hyp_text_normalized)\n",
                "    char_error_counts = {\n",
                "        \"Correct\": char_details.hits,\n",
                "        \"Substitution\": char_details.substitutions,\n",
                "        \"Deletion\": char_details.deletions,\n",
                "        \"Insertion\": char_details.insertions,\n",
                "    }\n",
                "\n",
                "    # ‚úÖ NEW: Speaker similarity (voice cloning)\n",
                "    speaker_similarity = None\n",
                "    if ref_audio_path is not None:\n",
                "        if speaker_sim_model is None:\n",
                "            speaker_sim_model = get_speaker_sim_model()  # lazy init\n",
                "        speaker_similarity = speaker_sim_model.cosine_similarity(str(ref_audio_path), str(audio_path))\n",
                "\n",
                "    # Prepare result directory\n",
                "    result_path = Path(result_dir)\n",
                "    result_path.mkdir(parents=True, exist_ok=True)\n",
                "\n",
                "    # Create JSON filename with timestamp\n",
                "    timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
                "    audio_name = Path(audio_path).stem\n",
                "    json_filename = f\"{model_name}_{audio_name}_metrics.json\"\n",
                "    json_path = result_path / json_filename\n",
                "\n",
                "    # Save results to JSON\n",
                "    results_to_save = {\n",
                "        \"model_name\": model_name,\n",
                "        \"audio_file\": str(audio_path),\n",
                "        \"reference\": ref_text,\n",
                "        \"hypothesis\": hyp_text,\n",
                "        \"WER\": wer_score,\n",
                "        \"CER\": cer_score,\n",
                "        \"word_error_breakdown\": word_error_counts,\n",
                "        \"char_error_breakdown\": char_error_counts,\n",
                "        \"speaker_ref_audio\": str(ref_audio_path) if ref_audio_path is not None else None,  # ‚úÖ NEW\n",
                "        \"speaker_similarity_cosine\": speaker_similarity,                                   # ‚úÖ NEW\n",
                "        \"asr_model\": asr_model_name,\n",
                "        \"timestamp\": timestamp\n",
                "    }\n",
                "\n",
                "    with open(json_path, 'w', encoding='utf-8') as f:\n",
                "        json.dump(results_to_save, f, ensure_ascii=False, indent=2)\n",
                "\n",
                "    print(f\"üíæ Saved metrics (WER+CER+SIM) to: {json_path}\")\n",
                "    return str(json_path)\n",
                "\n",
                "def plot_comparison(json_paths, result_dir=\"../results\", chart_filename=None):\n",
                "    \"\"\"\n",
                "    V·∫Ω ƒë·ªì th·ªã so s√°nh WER + CER + Speaker Similarity (n·∫øu c√≥)\n",
                "    \"\"\"\n",
                "    # Load all JSON data\n",
                "    all_results = []\n",
                "    for json_path in json_paths:\n",
                "        with open(json_path, 'r', encoding='utf-8') as f:\n",
                "            data = json.load(f)\n",
                "            all_results.append(data)\n",
                "\n",
                "    model_names = [r.get(\"model_name\", \"Unknown\") for r in all_results]\n",
                "    wer_scores = [r.get(\"WER\", 0) * 100 for r in all_results]  # %\n",
                "    cer_scores = [r.get(\"CER\", 0) * 100 for r in all_results]  # %\n",
                "    sim_scores = [r.get(\"speaker_similarity_cosine\", None) for r in all_results]  # float or None\n",
                "\n",
                "    # Create figure with 3 subplots\n",
                "    fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(20, 6))\n",
                "    colors = plt.cm.Set2(range(len(model_names)))\n",
                "\n",
                "    # Plot 1: WER\n",
                "    bars1 = ax1.bar(model_names, wer_scores, color=colors)\n",
                "    ax1.set_xlabel(\"Model TTS\")\n",
                "    ax1.set_ylabel(\"WER (%)\")\n",
                "    ax1.set_title(\"So s√°nh Word Error Rate (WER)\")\n",
                "    ax1.set_ylim(0, max(wer_scores) * 1.2 if wer_scores else 10)\n",
                "    for bar, score in zip(bars1, wer_scores):\n",
                "        ax1.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,\n",
                "                 f'{score:.2f}%', ha='center', va='bottom', fontsize=10)\n",
                "\n",
                "    # Plot 2: CER\n",
                "    bars2 = ax2.bar(model_names, cer_scores, color=colors)\n",
                "    ax2.set_xlabel(\"Model TTS\")\n",
                "    ax2.set_ylabel(\"CER (%)\")\n",
                "    ax2.set_title(\"So s√°nh Character Error Rate (CER)\")\n",
                "    ax2.set_ylim(0, max(cer_scores) * 1.2 if cer_scores else 10)\n",
                "    for bar, score in zip(bars2, cer_scores):\n",
                "        ax2.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.5,\n",
                "                 f'{score:.2f}%', ha='center', va='bottom', fontsize=10)\n",
                "\n",
                "    # Plot 3: Speaker Similarity\n",
                "    # N·∫øu thi·∫øu similarity (None) th√¨ v·∫Ω 0 v√† annotate \"N/A\"\n",
                "    sim_plot_vals = [(s if s is not None else 0.0) for s in sim_scores]\n",
                "    bars3 = ax3.bar(model_names, sim_plot_vals, color=colors)\n",
                "    ax3.set_xlabel(\"Model TTS\")\n",
                "    ax3.set_ylabel(\"Cosine Similarity\")\n",
                "    ax3.set_title(\"So s√°nh Speaker Similarity (Voice Cloning)\")\n",
                "    ax3.set_ylim(0, 1.0)\n",
                "\n",
                "    for bar, s in zip(bars3, sim_scores):\n",
                "        label = f\"{s:.3f}\" if isinstance(s, (float, int)) else \"N/A\"\n",
                "        ax3.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.02,\n",
                "                 label, ha='center', va='bottom', fontsize=10)\n",
                "\n",
                "    plt.tight_layout()\n",
                "\n",
                "    # Save figure\n",
                "    result_path = Path(result_dir)\n",
                "    result_path.mkdir(parents=True, exist_ok=True)\n",
                "\n",
                "    if chart_filename is None:\n",
                "        timestamp = datetime.datetime.now().strftime('%Y%m%d_%H%M%S')\n",
                "        chart_filename = f\"metrics_comparison.png\"\n",
                "\n",
                "    chart_path = result_path / chart_filename\n",
                "    plt.savefig(chart_path, dpi=150, bbox_inches='tight')\n",
                "    plt.close()\n",
                "\n",
                "    print(f\"üìä Saved comparison chart to: {chart_path}\")\n",
                "    return str(chart_path)\n",
                "\n",
                "def load_wer_results(json_path):\n",
                "    \"\"\"\n",
                "    ƒê·ªçc k·∫øt qu·∫£ WER t·ª´ file JSON\n",
                "    \n",
                "    Args:\n",
                "        json_path: str - ƒë∆∞·ªùng d·∫´n ƒë·∫øn file JSON\n",
                "    \n",
                "    Returns:\n",
                "        dict - k·∫øt qu·∫£ WER ƒë√£ l∆∞u\n",
                "    \"\"\"\n",
                "    with open(json_path, 'r', encoding='utf-8') as f:\n",
                "        return json.load(f)\n",
                "\n",
                "\n",
                "\n",
                "\n",
                "metric_calculate(\n",
                "    audio_path=\"/home/lamquy/Project/TTS/results/F5-TTS-Vietnamese/20260116_143338/sample_1.wav\", \n",
                "    # audio_path=\"/home/lamquy/Project/TTS/results/XTTSv2/20260116_143925.wav\", \n",
                "    # audio_path=\"/home/lamquy/Project/TTS/results/VieNeu-TTS/20260116_142158/sample_1.wav\", \n",
                "    ref_audio_path=\"/home/lamquy/Project/TTS/F5-TTS-Vietnamese/samples/khoi/khoi.wav\",\n",
                "    ref_text=\"H√† N·ªôi, tr√°i tim c·ªßa Vi·ªát Nam, l√† m·ªôt th√†nh ph·ªë ng√†n nƒÉm vƒÉn hi·∫øn v·ªõi b·ªÅ d√†y l·ªãch s·ª≠ v√† vƒÉn h√≥a ƒë·ªôc ƒë√°o. B∆∞·ªõc ch√¢n tr√™n nh·ªØng con ph·ªë c·ªï k√≠nh quanh H·ªì Ho√†n Ki·∫øm, du kh√°ch nh∆∞ ƒë∆∞·ª£c du h√†nh ng∆∞·ª£c th·ªùi gian, chi√™m ng∆∞·ª°ng ki·∫øn tr√∫c Ph√°p c·ªï ƒëi·ªÉn h√≤a quy·ªán v·ªõi n√©t ki·∫øn tr√∫c truy·ªÅn th·ªëng Vi·ªát Nam. M·ªói con ph·ªë trong khu ph·ªë c·ªï mang m·ªôt t√™n g·ªçi ƒë·∫∑c tr∆∞ng, ph·∫£n √°nh ngh·ªÅ th·ªß c√¥ng truy·ªÅn th·ªëng t·ª´ng th·ªãnh h√†nh n∆°i ƒë√¢y nh∆∞ ph·ªë H√†ng B·∫°c, H√†ng ƒê√†o, H√†ng M√£. ·∫®m th·ª±c H√† N·ªôi c≈©ng l√† m·ªôt ƒëi·ªÉm nh·∫•n ƒë·∫∑c bi·ªát, t·ª´ t√¥ ph·ªü n√≥ng h·ªïi bu·ªïi s√°ng, b√∫n ch·∫£ th∆°m l·ª´ng tr∆∞a h√®, ƒë·∫øn ch√® Th√°i ng·ªçt ng√†o chi·ªÅu thu. Nh·ªØng m√≥n ƒÉn d√¢n d√£ n√†y ƒë√£ tr·ªü th√†nh bi·ªÉu t∆∞·ª£ng c·ªßa vƒÉn h√≥a ·∫©m th·ª±c Vi·ªát, ƒë∆∞·ª£c c·∫£ th·∫ø gi·ªõi y√™u m·∫øn. Ng∆∞·ªùi H√† N·ªôi n·ªïi ti·∫øng v·ªõi t√≠nh c√°ch hi·ªÅn h√≤a, l·ªãch thi·ªáp nh∆∞ng c≈©ng r·∫•t c·∫ßu to√†n trong t·ª´ng chi ti·∫øt nh·ªè, t·ª´ c√°ch pha tr√† sen cho ƒë·∫øn c√°ch ch·ªçn hoa sen t√¢y ƒë·ªÉ th∆∞·ªüng tr√†.\", \n",
                "    model_name=\"F5-TTS-Vietnamese\", \n",
                "    asr_model_name=\"large\", \n",
                "    language=\"vi\", \n",
                "    result_dir=\"../results/json\"\n",
                ")\n",
                "\n",
                "# plot_comparison([\n",
                "#     \"/home/lamquy/Project/TTS/results/json/F5-TTS_sample_1_metrics.json\",\n",
                "#     \"/home/lamquy/Project/TTS/results/json/VieNeu-TTS_sample_2_metrics.json\",\n",
                "#     \"/home/lamquy/Project/TTS/results/json/XTTSv2_20260116_114606_metrics.json\"\n",
                "# ])"
            ]
        },
        {
            "cell_type": "code",
            "execution_count": 8,
            "id": "run-default",
            "metadata": {},
            "outputs": [
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "üöÄ Initializing VieNeu SDK...\n",
                        "Loading backbone from: pnnbao-ump/VieNeu-TTS-0.3B-q4-gguf on cpu ...\n"
                    ]
                },
                {
                    "name": "stderr",
                    "output_type": "stream",
                    "text": [
                        "llama_context: n_ctx_per_seq (2048) < n_ctx_train (4096) -- the full capacity of the model will not be utilized\n"
                    ]
                },
                {
                    "name": "stdout",
                    "output_type": "stream",
                    "text": [
                        "Loading codec from: neuphonic/distill-neucodec on cpu ...\n",
                        "‚úÖ SDK initialized successfully\n",
                        "üìã Available preset voices: ['Binh', 'Tuyen', 'Vinh', 'Doan', 'Ly', 'Ngoc']\n",
                        "‚úÖ Selected voice: Binh\n",
                        "üéôÔ∏è Cloning voice from: example_ngoc_huyen.wav\n",
                        "‚úÖ Voice 'ngoc_huyen' saved to /home/lamquy/Project/TTS/VieNeu-TTS/vieneu/assets/samples\n",
                        "‚úÖ Voice cloned and saved as: 'ngoc_huyen'\n",
                        "üìã Updated voice list: ['Binh', 'Tuyen', 'Vinh', 'Doan', 'Ly', 'Ngoc', 'ngoc_huyen']\n",
                        "üìÅ Output directory: /home/lamquy/Project/TTS/results/VieNeu-TTS/20260116_142158\n",
                        "üìù Total samples to process: 1\n",
                        "\n",
                        "üéß Sample 1/1: H√† N·ªôi, tr√°i tim c·ªßa Vi·ªát Nam, l√† m·ªôt th√†nh ph·ªë ng...\n",
                        "   üíæ Saved: /home/lamquy/Project/TTS/results/VieNeu-TTS/20260116_142158/sample_1.wav\n",
                        "\n",
                        "‚úÖ All samples processed successfully!\n",
                        "‚úÖ TTS engine closed\n",
                        "[PosixPath('/home/lamquy/Project/TTS/results/VieNeu-TTS/20260116_142158/sample_1.wav')]\n"
                    ]
                }
            ],
            "source": [
                "TEXT_SAMPLES = [\n",
                "    # B·∫£ng ch·ªØ c√°i ti·∫øng Vi·ªát\n",
                "    # \"A ƒÇ √Ç B C D ƒê E √ä G H I K L M N O √î ∆† P Q R S T U ∆Ø V X Y\",\n",
                "    \n",
                "    # # ƒêo·∫°n vƒÉn d√†i - test ƒë·ªô ·ªïn ƒë·ªãnh\n",
                "    # \"Ti·∫øng Vi·ªát l√† ng√¥n ng·ªØ gi√†u thanh ƒëi·ªáu v√† h√¨nh ·∫£nh, ph·∫£n √°nh ƒë·ªùi s·ªëng tinh t·∫ø c·ªßa con ng∆∞·ªùi Vi·ªát Nam. Trong giao ti·∫øp h·∫±ng ng√†y, ch√∫ng ta s·ª≠ d·ª•ng ti·∫øng Vi·ªát ƒë·ªÉ chia s·∫ª c·∫£m x√∫c, truy·ªÅn ƒë·∫°t th√¥ng tin v√† k·∫øt n·ªëi c·ªông ƒë·ªìng. Ng√¥n ng·ªØ n√†y kh√¥ng ch·ªâ c√≥ t·ª´ v·ª±ng phong ph√∫ m√† c√≤n c√≥ h·ªá th·ªëng d·∫•u thanh ƒë·∫∑c tr∆∞ng, gi√∫p c√¢u n√≥i tr·ªü n√™n sinh ƒë·ªông, r√µ nghƒ©a v√† c·∫£m x√∫c. Vi·ªác b·∫£o t·ªìn v√† ph√°t tri·ªÉn ti·∫øng Vi·ªát l√† tr√°ch nhi·ªám chung c·ªßa x√£ h·ªôi trong th·ªùi ƒë·∫°i s·ªë h√≥a nay.\",\n",
                "    \n",
                "    # # T√™n ri√™ng v√† ƒë·ªãa danh\n",
                "    # \"Nguy·ªÖn √Åi Qu·ªëc ƒë√£ t·ª´ng vi·∫øt v·ªÅ nh·ªØng chuy·∫øn chu du d√†i d·∫±ng d·∫∑c qua ch√¢u √Çu gi·ªØa m√πa ƒë√¥ng r√©t m∆∞·ªõt.\",\n",
                "    \n",
                "    # # Ph·ª• √¢m kh√≥ (ch, tr, s, x)\n",
                "    # \"Ch·ªã Tr√∫c nh·∫∑t nh·∫°nh t·ª´ng chi·∫øc ch√©n s·ª© s·ª©t s·∫πo tr√™n chi·∫øc ch√µng tre tr∆∞·ªõc hi√™n nh√†.\",\n",
                "    \n",
                "    # # C√¢u ng·∫Øn\n",
                "    # \"ƒê√¢y l√† ch·ªØ g\",\n",
                "    # \"N·∫øu b·∫°n kh√¥ng bi·∫øt m√¨nh ƒëang ·ªü ƒë√¢u, th√¨ b·∫•t c·ª© con ƒë∆∞·ªùng n√†o c≈©ng s·∫Ω d·∫´n b·∫°n ƒë·∫øn ƒë√≥.\",\n",
                "    # \"M·ªçi th·ª© ƒë√£ h√≥a ƒëi√™n v·ªõi ch·ªìng ng√†y h√¥m nay\",\n",
                "    \"H√† N·ªôi, tr√°i tim c·ªßa Vi·ªát Nam, l√† m·ªôt th√†nh ph·ªë ng√†n nƒÉm vƒÉn hi·∫øn v·ªõi b·ªÅ d√†y l·ªãch s·ª≠ v√† vƒÉn h√≥a ƒë·ªôc ƒë√°o. B∆∞·ªõc ch√¢n tr√™n nh·ªØng con ph·ªë c·ªï k√≠nh quanh H·ªì Ho√†n Ki·∫øm, du kh√°ch nh∆∞ ƒë∆∞·ª£c du h√†nh ng∆∞·ª£c th·ªùi gian, chi√™m ng∆∞·ª°ng ki·∫øn tr√∫c Ph√°p c·ªï ƒëi·ªÉn h√≤a quy·ªán v·ªõi n√©t ki·∫øn tr√∫c truy·ªÅn th·ªëng Vi·ªát Nam. M·ªói con ph·ªë trong khu ph·ªë c·ªï mang m·ªôt t√™n g·ªçi ƒë·∫∑c tr∆∞ng, ph·∫£n √°nh ngh·ªÅ th·ªß c√¥ng truy·ªÅn th·ªëng t·ª´ng th·ªãnh h√†nh n∆°i ƒë√¢y nh∆∞ ph·ªë H√†ng B·∫°c, H√†ng ƒê√†o, H√†ng M√£. ·∫®m th·ª±c H√† N·ªôi c≈©ng l√† m·ªôt ƒëi·ªÉm nh·∫•n ƒë·∫∑c bi·ªát, t·ª´ t√¥ ph·ªü n√≥ng h·ªïi bu·ªïi s√°ng, b√∫n ch·∫£ th∆°m l·ª´ng tr∆∞a h√®, ƒë·∫øn ch√® Th√°i ng·ªçt ng√†o chi·ªÅu thu. Nh·ªØng m√≥n ƒÉn d√¢n d√£ n√†y ƒë√£ tr·ªü th√†nh bi·ªÉu t∆∞·ª£ng c·ªßa vƒÉn h√≥a ·∫©m th·ª±c Vi·ªát, ƒë∆∞·ª£c c·∫£ th·∫ø gi·ªõi y√™u m·∫øn. Ng∆∞·ªùi H√† N·ªôi n·ªïi ti·∫øng v·ªõi t√≠nh c√°ch hi·ªÅn h√≤a, l·ªãch thi·ªáp nh∆∞ng c≈©ng r·∫•t c·∫ßu to√†n trong t·ª´ng chi ti·∫øt nh·ªè, t·ª´ c√°ch pha tr√† sen cho ƒë·∫øn c√°ch ch·ªçn hoa sen t√¢y ƒë·ªÉ th∆∞·ªüng tr√†.\",\n",
                "    \n",
                "\n",
                "]\n",
                "\n",
                "\"\"\"\n",
                "    Run batch TTS synthesis using VieNeu SDK.\n",
                "    \n",
                "    Args:\n",
                "        text_samples (list): List of text strings to synthesize. If None, will import from text_sample.py\n",
                "        output_base_dir (str|Path): Base directory for output files\n",
                "        notebook_dir (str|Path): Directory containing the notebook and examples\n",
                "        preset_voice (str): Name of preset voice to use (e.g., \"Binh\")\n",
                "        use_custom_voice (bool): Whether to clone and use a custom voice\n",
                "        sample_audio_path (str|Path): Path to sample audio for voice cloning\n",
                "        sample_audio_text (str): Transcript of the sample audio\n",
                "        custom_voice_name (str): Name to save the cloned voice as\n",
                "        temperature (float): Temperature for synthesis (0.1=stable, 1.0+=expressive)\n",
                "        top_k (int): Top-k sampling parameter\n",
                "        sample_rate (int): Audio sample rate for output files\n",
                "        \n",
                "    Returns:\n",
                "        Path: Directory containing the generated audio files\n",
                "    \"\"\"\n",
                "output_dir = run_vieneu_tts_batch(\n",
                "    text_samples= TEXT_SAMPLES,\n",
                "    output_base_dir=\"/home/lamquy/Project/TTS/results/VieNeu-TTS\",\n",
                "    notebook_dir=\"/home/lamquy/Project/TTS/VieNeu-TTS\",\n",
                "    # preset_voice=\"TTT2\",\n",
                "    use_custom_voice=True,\n",
                "    sample_audio_path=\"/home/lamquy/Project/TTS/VieNeu-TTS/examples/audio_ref/example_ngoc_huyen.wav\",\n",
                "    sample_audio_text_path=\"/home/lamquy/Project/TTS/VieNeu-TTS/examples/audio_ref/example_ngoc_huyen.txt\",\n",
                "    custom_voice_name=\"ngoc_huyen\",\n",
                "    temperature=1.0,\n",
                "    top_k=50,   \n",
                "    sample_rate=24000\n",
                ")\n",
                "print(output_dir)\n",
                "\n"
            ]
        }
    ],
    "metadata": {
        "kernelspec": {
            "display_name": "vienu",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "file_extension": ".py",
            "mimetype": "text/x-python",
            "name": "python",
            "nbconvert_exporter": "python",
            "pygments_lexer": "ipython3",
            "version": "3.11.14"
        }
    },
    "nbformat": 4,
    "nbformat_minor": 5
}
